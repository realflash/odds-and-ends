{
  "name": "cookie-client",
  "version": "0.0.3",
  "author": {
    "name": "Geraint Luff"
  },
  "description": "Basic cookie-handling for outgoing requests",
  "dependencies": {},
  "keywords": [
    "cookie",
    "client"
  ],
  "maintainers": [
    {
      "name": "Geraint Luff",
      "email": "luffgd@gmail.com",
      "url": "https://github.com/geraintluff/"
    }
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/geraintluff/cookie-client.git"
  },
  "main": "cookie-client.js",
  "license:": [
    {
      "type": "Public Domain",
      "url": "http://geraintluff.github.io/tv4/LICENSE.txt"
    },
    {
      "type": "MIT",
      "url": "http://jsonary.com/LICENSE.txt"
    }
  ],
  "readme": "# Cookie client\r\n\r\nThis library emulates client cookie behaviour, allowing appropriate cookie behaviour when making outgoing requests from Node.\r\n\r\nThis library is **not particularly efficient** for large numbers of cookies.  The cookies are stored in a big list, and this entire list is searched for every request.  For small numbers of cookies, or cookies on a single domain, this should not be an issue - so if you're just automatically logging into and exploring a single API, this should be fine.\r\n\r\nIf you're looking to create a full-blown multi-domain scraper or something, then raise a GitHub issue or email the author about making it more efficient.\r\n\r\n## Usage\r\n\r\n### Create a cookie store\r\n\r\n```javascript\r\nvar cookieClient = require('cookie-client');\r\n\r\nvar cookieStore = cookieClient();  // use of \"new\" is optional\r\n```\r\n\r\n### Adding cookies from incoming response headers\r\n\r\n```javascript\r\ncookieStore.addFromHeaders(response.headers); // full headers object\r\ncookieStore.addFromHeaders(response.headers['set-cookie']); // just the cookie headers\r\n```\r\n\r\n### Cookie string for outgoing request\r\n\r\n```javascript\r\nrequest.headers['cookie'] = cookieStore.cookieStringForRequest(domain, path, isSecure);\r\n```\r\n\r\n### Inspect cookie objects for outgoing request\r\n\r\n```javascript\r\nrequest.headers['cookie'] = cookieStore.cookiesForRequest(domain, path, isSecure);\r\n```\r\n\r\n## Public Suffix List\r\n\r\nTo prevent \"super-cookies\" assigning themselves domains like `.com` (which is dangerous), this module attempts to download a Public Suffix List (see [publicsuffix.org](http://publicsuffix.org/)).  Any response other than a 200 will log an error to the console.\r\n\r\nThis is downloaded and stored in the module directory, so it happens once per installation.\r\n\r\nEven if it is cached locally, it is loaded asynchronously.  You can query whether the PSL has loaded yet, or request a callback:\r\n\r\n```javascript\r\ncookieClient.pslLoaded;  // boolean flag\r\n\r\ncookieClient,whenPslLoaded(function (error) {\r\n\t...\r\n});\r\n```\r\n\r\n## License\r\n\r\nThe code is licensed as [Public Domain](http://geraintluff.github.io/tv4/LICENSE.txt) or [MIT](http://jsonary.com/LICENSE.txt) (your choice).\r\n\r\nHowever, the file `public-suffix-list.txt` is taken from [publicsuffix.org](http://publicsuffix.org/), and has separate licensing terms (Mozilla Public License).  This package *used* to fetch the file from the web on first run, but that caused Node to crash if the connection was dropped by the remote server.  It is therefore included in the package, but is license separately from the rest of the code.",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/geraintluff/cookie-client/issues"
  },
  "_id": "cookie-client@0.0.3",
  "dist": {
    "shasum": "00ad30890ec9a60667f142fa9f1b3382dd55d253"
  },
  "_from": "cookie-client@",
  "_resolved": "https://registry.npmjs.org/cookie-client/-/cookie-client-0.0.3.tgz"
}
